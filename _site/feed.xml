<?xml version="1.0" encoding="utf-8"?>

<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kadie Jaffe</title>
    <description></description>
    <link>http://kadie16.github.io/</link>
    <atom:link href="http://kadie16.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 31 Mar 2016 23:45:56 -0700</pubDate>
    <lastBuildDate>Thu, 31 Mar 2016 23:45:56 -0700</lastBuildDate>
    <generator>Jekyll v3.1.2</generator>
    
      <item>
        <title>Raytracer</title>
        <description>&lt;div&gt;
        &lt;p&gt; In this project, I wrote a ray tracer! Ray tracing involves tracing the path of light rays to generate images. By tracing the path of many many rays, I can determine how objects in a scene interact with eachother. For example, I can trace a bunch of rays from a light source, to an object, to the camera, determine how the object appears in the scene, and render or &quot;draw&quot; it to the screen accordingly.&lt;/p&gt;
        &lt;img src=&quot;../images/p3/part4/CBbunny_hd.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;You have to read the whole thing to appreciate how much effort went into rendering this bunny :)!&lt;/figcaption&gt;

    &lt;h2 align=&quot;middle&quot;&gt;Part 1: Ray Generation and Scene Intersection &lt;/h2&gt;
        &lt;p&gt;The first thing I did was implement a raytrace_pixel() method. The method integrates the irradiance of the pixel.  &lt;/p&gt;
        &lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Irradiance:&lt;/strong&gt; &lt;br /&gt; 
The flux of radiant energy per unit area (normal to the direction of flow of radiant energy through a medium)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, I implemented raytrace_pixel() to find the amount of light energy in the given pixel, which tells me how bright the pixel should be. I did this by generating a given number of &quot;rays&quot; through the pixel. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Ray:&lt;/strong&gt; &lt;br /&gt; 
A line with a start point, but no end point. &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; In my program, a ray is given an origin (it&#39;s start point), a vector which dictates it&#39;s direction, a depth, and a &quot;min_t&quot; and &quot;max_t&quot;. The &quot;min_t&quot; and &quot;max_t&quot; allow me to treat the ray as a segment, if I wish. So I can cut it off at the minimum time and maximum time I set. This becomes important later! &lt;/p&gt;
&lt;p&gt; In this case, all of the rays I generate have the &quot;camera&quot;&#39;s position as their origin and a vector towards the pixel as their direction. You can think of the camera as where your eye is in the scene. So basically I am drawing a bunch of lines from my eyeball to a point in space and summing up all of the radiant energy, or irradiance, that my eyeball is detecting at that point in space.&lt;/p&gt;
&lt;h3&gt; Primitive Intersections &lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Primitive:&lt;/strong&gt; &lt;br /&gt; 
These are the building blocks of the scenes. Triangles are used most commonly, but all kinds of shapes can be used! &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; Next, I used the Moller Trumbore algorithm to implement a method, Triangle::intersect(), that lets me know if a given Ray intersects that particular triangle. The Moller Trumbore algorithm gives a way to move the origin of the ray and change the base of it to get a vector: [t, u, v] where t represents the distance from the ray&#39;s original origin to the plane the triangle is on, and u and v are the Barycentric coordinates within the triangle that the ray intersects. (See part 5 of my &lt;a href=&quot;/rasterizester&quot;&gt; rasterizester &lt;/a&gt; project for more on Barycentric coordinates). 

&lt;p&gt; Since most of the scenes I will be rendering are made up of triangles, it is super important to be able to tell whether the Rays I trace are intersecting them. If I trace a ray from a light&#39;s position out in some direction and it intersects a triangle, T1, I can tell exactly which point on that triangle T1 is lit up by that particular ray... almost. What if there is another, larger triangle T2 in between the light and T1?? Then the point I found on T1 wouldn&#39;t be lit up at all, since it would be obstructed by T2.&lt;/p&gt;

&lt;p&gt; Here is where &quot;min_t&quot; and &quot;max_t&quot; come in. When I trace a ray from a light and use the Moller Trumbore algorithm to test whether it is intersecting the primitives triangles, T1 and T2, in my scene, I will find out it intersects both of them and the &quot;t&#39;s&quot; (distances) the triangles were from the origin of the ray. However, I am only interested in the closest intersection since that is the one which is lit! So, once the ray intersects T2, I set the ray&#39;s &quot;max_t&quot; to the distance T2 is from the origin of the ray. That way, when I test T1, I will be able to tell it isn&#39;t the closest intersection, since it&#39;s t value will be greater than the ray&#39;s &quot;max_t&quot;. &lt;/p&gt;

&lt;p&gt; I implemented a similar method for intersecting spheres, and then I was able to render my first images! &lt;/p&gt;

&lt;div&gt;
            &lt;img src=&quot;../images/p3/part_1/spheres.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Spheres&lt;/figcaption&gt;

            &lt;img src=&quot;../images/p3/part_1/coil.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Coil&lt;/figcaption&gt;

&lt;h2 align=&quot;middle&quot;&gt;Part 2: Bounding Volume Hierarchy (BVH)&lt;/h2&gt;
&lt;p&gt; So, remember that all of the scenes I am rendering are made up of a bunch of &quot;primitives&quot;, usually triangles. As you can imagine, really complicated scenes require A LOT of triangles. We could be talking on the scale of hundreds of thousands of triangles, or even more. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/wall-e_tri.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Look at all of those triangles!&lt;/figcaption&gt;
&lt;p&gt; With that in mind, it isn&#39;t efficient to loop through every single triangle of the mesh for every ray I need to trace. I am tracing potentially thousands of rays per primative per scene. For simple meshes like those in part one, that will suffice, but for more complicated meshes like Wall-E I need a better solution. This is where bounding volume heirarchys come in. &lt;/p&gt;
&lt;p&gt; By subdividing my mesh into bounding volumes, I can first test if a ray intersects a large bounding volume containg part of the mesh. If it doesn&#39;t, I just saved a bunch of time. Before, I would have had to test all of the primitives in that volume! If it does, I can descend into the smallest bounding volume the ray intersects, and test only the primitives in that box. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh0.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;The boxes are the bounding volumes that divide the different pieces of Wall-E &lt;/figcaption&gt;
&lt;h3&gt; Constructing the BVH &lt;/h3&gt;
&lt;p&gt; To construct the BVH, I find a bounding box that contains all of the primitives in the scene. Then, I create an empty BVH node. If the amount of primitives in the box are under a specified max_leaf_size, then I am done. Otherwise, I need to divide that bounding box into smaller ones. &lt;/p&gt; 
&lt;p&gt; To accomplish this, I split the current bounding box in half. It is a 3D bounding volume, so I split it along it&#39;s longest dimension. Then, I put each of the primitives into either the left or right box. If their centroid in the chosen axis is less than the split point, they go in the left box, and vice versa. This continues recursively until all of the primitives are contained in boxes with &amp;lt;= max_leaf_size total primitives. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh2.png&quot; /&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Showing some smaller bounding volumes in Wall-E&#39;s BVH tree&lt;/figcaption&gt;
&lt;p&gt; Now if I am testing a ray which intersects a primitive Wall-E&#39;s hand, I save myself a bunch of work, since I don&#39;t have to loop through any of the primitives outside of the little bounding volume that contains his hand. Again, this is super important, because sometimes I am tracing thousands of rays per pixel!&lt;/p&gt;
&lt;h3&gt; Intersecting a Bounding Volume &lt;/h3&gt;
&lt;p&gt; To find intersections in my bounding volume, I start with the ray and the root node of my bvh tree (the giant box that holds all of the primitives). &lt;/p&gt;
    &lt;li&gt; If the ray doesn&#39;t intersect the root node, I&#39;m done. I know it can&#39;t intersect any of the triangles inside the box if it doesn&#39;t even intersect the box itself! 
    &lt;li&gt; If the node is a leaf (it contains less than max_leaf_size primitives), then I test if the ray intersects all of the primitives inside the leaf.
    &lt;li&gt; Otherwise, I recursively check the node&#39;s children left and right boxes. 
    &lt;/li&gt;
&lt;p&gt; This continues until I reach a bounding volume the ray doesn&#39;t intersect, or until I reach the leaf node and test the primitives inside. &lt;/p&gt;
        &lt;img src=&quot;../images/p3/part_2/walle.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Now I can render super big meshes like Wall-E!&lt;/figcaption&gt;
        &lt;img src=&quot;../images/p3/part_2/max.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;And Max Planck!&lt;/figcaption&gt;


 &lt;h2 align=&quot;middle&quot;&gt;Part 3: Direct Illumination&lt;/h2&gt;
&lt;p&gt;Up until this part, I was only tracing camera rays. The lighting in the scenes was just based on the normal vectors from the primitive to the camera. Now I will start tracing rays from lights to the scene to determine the radiance of each pixel!&lt;/p&gt; &lt;p&gt; To determine the direct lighting of the scene, I sum over all of the light sources in the scene. From each light, I take sample rays and compute the incoming radiance from those directions. Then I convert the incoming radiance to outgoing radiance using the Bidirectional Scattering Distribution Function (BSDF) of the surface. &lt;/p&gt;

        &lt;img src=&quot;../images/p3/part3/dragon_l1.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Dragon with 1 sample per Light Source &lt;br /&gt; The image is very noisy!&lt;/figcaption&gt;
        &lt;img src=&quot;../images/p3/part3/dragon_l16.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Dragon with 16 samples per Light Source &lt;br /&gt; Notice the shadows on the dragon&#39;s neck and tail are much more distinct! &lt;/figcaption&gt; 
&lt;br /&gt;

 &lt;h2 align=&quot;middle&quot;&gt;Part 4: Indirect Illumination&lt;/h2&gt;
&lt;p&gt;Now, instead of just considering light rays directly from the light to the surface, I let the rays &quot;bounce&quot;. This lets me detect radiance that is coming not only directly from the light, but from light bouncing off the other objects! &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part4/bunny_indirect.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with only Indirect Lighting ... &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/bunny_direct.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with only Direct Lighting &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/CBbunny_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with Global Lighting (Indirect and Direct) &lt;br /&gt; Notice the colors visible in the shadows!&lt;/figcaption&gt; 
            &lt;br /&gt; 
&lt;p&gt; To accomplish this, first I take a sample from the surface BSDF at the point where the ray hits. Using the illumination from that sample, I determine whether to stop there or let the ray bounce again. I can&#39;t just let the rays bounce forever (too expensive!), but fortunately I can achieve a pretty realistic result using &quot;Russian Roulette&quot; to decide how whether or not to terminate the ray. I flip a biased coin with probability proportional to illumination of returning true. If the illumination is already very low, I am more likely to terminate the ray. If it is very high, I am more likely to let it keep bouncing. This way even though I am sampling &quot;randomly&quot;, I am making sure the brightest rays are being represented. &lt;/p&gt;
&lt;p&gt; If the coinflip returns true, I recursively trace the ray again, offsetting it&#39;s origin slightly and sending it off in the direction of the incoming radiance converted to world coordinates. The light from the ray is accumulated in the scene until the russian roulette terminates it. &lt;/p&gt; 
&lt;h3&gt; Global Illumination at varying Sample Rates &lt;/h3&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 1 Sample per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s4.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 4 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s16.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 16 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s64.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 64 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 1024 Samples per Pixel &lt;/figcaption&gt;
            &lt;br /&gt;
&lt;p&gt; As you can see, it takes a lot of samples per pixel to completely eliminate the noise in the image! Images with only one sample per pixel take seconds to render, while images with 1024 samples take hours! &lt;/p&gt;
&lt;h3&gt; Varying Camera Ray Depths &lt;/h3&gt;
&lt;p&gt; This is skipping ahead a little, but once I add the glass and metallic materials, you can clearly see the effects of increasing the max ray depth. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 1. &lt;br /&gt; Rays enter the glass sphere but they never leave! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m2.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 2. &lt;br /&gt; Now the rays bounce inside the glass! &lt;br /&gt; You can tell they are still trapped inside because the mirror ball still shows a dark glass sphere. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m3.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 3. &lt;br /&gt; Finally the rays are making it outside of the glass. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m4.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 4. &lt;br /&gt; Now the rays are showing up on the right wall too! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 100. &lt;/figcaption&gt; &lt;br /&gt; 


&lt;h2 align=&quot;middle&quot;&gt;Part 5: Materials&lt;/h2&gt;
&lt;img src=&quot;../images/p3/part5/spheres2_100_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Mirror and Glass Ball &lt;/figcaption&gt;
&lt;p&gt; Remember the Bidirectional Scattering Distribution Functions I mentioned earlier? They take convert the incoming radiance of a ray to outgoing radiance. Different materials have different BSDFs, since light reacts differently to them. I implemented glass and mirror BSDFs to generate the image you see above.&lt;/p&gt;  
&lt;h3&gt; Mirror and Glass BSDF &lt;/h3&gt;
&lt;p&gt; The mirror ball (left) requires a mirror BSDF. This was relatively simple to implement, since the ray just reflects off of the mirror. The glass ball (right) is much more complicated. Sometimes light reflects off glass, and sometimes it refracts inside! To simulate this, I used another biased coin flip based off of the Schlick&#39;s Approximation. If the coin returned true, I reflected the incoming radiance as I did with the mirror BSDF, except with the Schlick coefficient as the probability density function (for mirror the probability density function was 1). Otherwise, I refracted the incoming radiance. &lt;/p&gt;

            &lt;img src=&quot;../images/p3/part5/spheres2_1_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 1. &lt;br /&gt; Rays enter the glass sphere but they never leave! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_2_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 2. &lt;br /&gt; Now the rays bounce inside the glass! &lt;br /&gt; You can tell they are still trapped inside because the mirror ball still shows a dark glass sphere. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_3_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 3. &lt;br /&gt; Finally the rays are making it outside of the glass. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_4_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 4. &lt;br /&gt; Now the rays are showing up on the right wall, and bouncing back onto the glass! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_100_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 100. &lt;/figcaption&gt; &lt;br /&gt; 

&lt;h3&gt; Increasing Samples per Pixel &lt;/h3&gt;
&lt;figcaption align=&quot;middle&quot;&gt; One Sample per Light and max_ray_depth = 100 &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s1_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 1 sample per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s4_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 4 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s16_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 16 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s64_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 64 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s1024_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 1024 samples per pixel &lt;/figcaption&gt; &lt;br /&gt; 



&lt;h2 align=&quot;middle&quot;&gt; Sick Renders! &lt;/h2&gt;
&lt;figcaption align=&quot;middle&quot;&gt; Here are my favorite renders! All with 1024 Samples/Pixel.&lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/fun/dragon_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Mirror Dragon &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/fun/CBgems_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Gems &lt;/figcaption&gt;




&lt;/li&gt;&lt;/li&gt;&lt;/div&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Mar 2016 00:00:00 -0700</pubDate>
        <link>http://kadie16.github.io/raytracer/</link>
        <guid isPermaLink="true">http://kadie16.github.io/raytracer/</guid>
        
        
        <category>projects</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>Raytracer</title>
        <description>&lt;div&gt;
        &lt;p&gt; In this project, I wrote a ray tracer! Ray tracing involves tracing the path of light rays to generate images. By tracing the path of many many rays, I can determine how objects in a scene interact with eachother. For example, I can trace a bunch of rays from a light source, to an object, to the camera, determine how the object appears in the scene, and render or &quot;draw&quot; it to the screen accordingly.&lt;/p&gt;

    &lt;h2 align=&quot;middle&quot;&gt;Part 1: Ray Generation and Scene Intersection &lt;/h2&gt;
        &lt;p&gt;The first thing I did was implement a raytrace_pixel() method. The method integrates the irradiance of the pixel.  &lt;/p&gt;
        &lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Irradiance:&lt;/strong&gt; &lt;br /&gt; 
The flux of radiant energy per unit area (normal to the direction of flow of radiant energy through a medium)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In other words, I implemented raytrace_pixel() to find the amount of light energy in the given pixel, which tells me how bright the pixel should be. I did this by generating a given number of &quot;rays&quot; through the pixel. &lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Ray:&lt;/strong&gt; &lt;br /&gt; 
A line with a start point, but no end point. &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; In my program, a ray is given an origin (it&#39;s start point), a vector which dictates it&#39;s direction, a depth, and a &quot;min_t&quot; and &quot;max_t&quot;. The &quot;min_t&quot; and &quot;max_t&quot; allow me to treat the ray as a segment, if I wish. So I can cut it off at the minimum time and maximum time I set. This becomes important later! &lt;/p&gt;
&lt;p&gt; In this case, all of the rays I generate have the &quot;camera&quot;&#39;s position as their origin and a vector towards the pixel as their direction. You can think of the camera as where your eye is in the scene. So basically I am drawing a bunch of lines from my eyeball to a point in space and summing up all of the radiant energy, or irradiance, that my eyeball is detecting at that point in space.&lt;/p&gt;
&lt;h3&gt; Primitive Intersections &lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Primitive:&lt;/strong&gt; &lt;br /&gt; 
These are the building blocks of the scenes. Triangles are used most commonly, but all kinds of shapes can be used! &lt;br /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt; Next, I used the Moller Trumbore algorithm to implement a method, Triangle::intersect(), that lets me know if a given Ray intersects that particular triangle. The Moller Trumbore algorithm gives a way to move the origin of the ray and change the base of it to get a vector: [t, u, v] where t represents the distance from the ray&#39;s original origin to the plane the triangle is on, and u and v are the Barycentric coordinates within the triangle that the ray intersects. (See part 5 of my &lt;a href=&quot;/rasterizester&quot;&gt; rasterizester &lt;/a&gt; project for more on Barycentric coordinates) 

&lt;p&gt; Since most of the scenes I will be rendering are made up of triangles, it is super important to be able to tell whether the Rays I trace are intersecting them. If I trace a ray from a light&#39;s position out in some direction and it intersects a triangle, T1, I can tell exactly which point on that triangle T1 is lit up by that particular ray... almost. What if there is another, larger triangle T2 in between the light and T1?? Then the point I found on T1 wouldn&#39;t be lit up at all, since it would be obstructed by T2.&lt;/p&gt;

&lt;p&gt; Here is where &quot;min_t&quot; and &quot;max_t&quot; come in. When I trace a ray from a light and use the Moller Trumbore algorithm to test whether it is intersecting the primitives triangles, T1 and T2, in my scene, I will find out it intersects both of them and the &quot;t&#39;s&quot; (distances) the triangles were from the origin of the ray. However, I am only interested in the closest intersection since that is the one which is lit! So, once the ray intersects T2, I set the ray&#39;s &quot;max_t&quot; to the distance T2 is from the origin of the ray. That way, when I test T1, I will be able to tell it isn&#39;t the closest intersection, since it&#39;s t value will be greater than the ray&#39;s &quot;max_t&quot;. &lt;/p&gt;

&lt;p&gt; I implemented a similar method for intersecting spheres, and then I was able to render my first images! &lt;/p&gt;

&lt;div&gt;
            &lt;img src=&quot;../images/p3/part_1/spheres.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Spheres&lt;/figcaption&gt;

            &lt;img src=&quot;../images/p3/part_1/coil.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Coil&lt;/figcaption&gt;

&lt;h2 align=&quot;middle&quot;&gt;Part 2: Bounding Volume Hierarchy (BVH)&lt;/h2&gt;
&lt;p&gt; So, remember that all of the scenes I am rendering are made up of a bunch of &quot;primitives&quot;, usually triangles. As you can imagine, really complicated scenes require A LOT of triangles. We could be talking on the scale of hundreds of thousands of triangles, or even more. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/wall-e_tri.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Look at all of those triangles!&lt;/figcaption&gt;
&lt;p&gt; With that in mind, it isn&#39;t efficient to loop through every single triangle of the mesh for every ray I need to trace. I am tracing potentially thousands of rays per primative per scene. For simple meshes like those in part one, that will suffice, but for more complicated meshes like Wall-E I need a better solution. This is where bounding volume heirarchys come in. &lt;/p&gt;
&lt;p&gt; By subdividing my mesh into bounding volumes, I can first test if a ray intersects a large bounding volume containg part of the mesh. If it doesn&#39;t, I just saved a bunch of time. Before, I would have had to test all of the primitives in that volume! If it does, I can descend into the smallest bounding volume the ray intersects, and test only the primitives in that box. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh0.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;The boxes are the bounding volumes that divide the different pieces of Wall-E &lt;/figcaption&gt;
&lt;h3&gt; Constructing the BVH &lt;/h3&gt;
&lt;p&gt; To construct the BVH, I find a bounding box that contains all of the primitives in the scene. Then, I create an empty BVH node. If the amount of primitives in the box are under a specified max_leaf_size, then I am done. Otherwise, I need to divide that bounding box into smaller ones. &lt;/p&gt; 
&lt;p&gt; To accomplish this, I split the current bounding box in half. It is a 3D bounding volume, so I split it along it&#39;s longest dimension. Then, I put each of the primitives into either the left or right box. If their centroid in the chosen axis is less than the split point, they go in the left box, and vice versa. This continues recursively until all of the primitives are contained in boxes with &amp;lt;= max_leaf_size total primitives. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh2.png&quot; /&gt;
            &lt;img src=&quot;../images/p3/part_2/walle_bvh1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Showing some smaller bounding volumes in Wall-E&#39;s BVH tree&lt;/figcaption&gt;
&lt;p&gt; Now if I am testing a ray which intersects a primitive Wall-E&#39;s hand, I save myself a bunch of work, since I don&#39;t have to loop through any of the primitives outside of the little bounding volume that contains his hand. Again, this is super important, because sometimes I am tracing thousands of rays per pixel!&lt;/p&gt;
&lt;h3&gt; Intersecting a Bounding Volume &lt;/h3&gt;
&lt;p&gt; To find intersections in my bounding volume, I start with the ray and the root node of my bvh tree (the giant box that holds all of the primitives). &lt;/p&gt;
    &lt;li&gt; If the ray doesn&#39;t intersect the root node, I&#39;m done. I know it can&#39;t intersect any of the triangles inside the box if it doesn&#39;t even intersect the box itself! 
    &lt;li&gt; If the node is a leaf (it contains less than max_leaf_size primitives), then I test if the ray intersects all of the primitives inside the leaf.
    &lt;li&gt; Otherwise, I recursively check the node&#39;s children left and right boxes. 
    &lt;/li&gt;
&lt;p&gt; This continues until I reach a bounding volume the ray doesn&#39;t intersect, or until I reach the leaf node and test the primitives inside. &lt;/p&gt;
        &lt;img src=&quot;../images/p3/part_2/walle.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Now I can render super big meshes like Wall-E!&lt;/figcaption&gt;
        &lt;img src=&quot;../images/p3/part_2/max.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;And Max Planck!&lt;/figcaption&gt;


 &lt;h2 align=&quot;middle&quot;&gt;Part 3: Direct Illumination&lt;/h2&gt;
&lt;p&gt;Up until this part, I was only tracing camera rays. The lighting in the scenes was just based on the normal vectors from the primitive to the camera. Now I will start tracing rays from lights to the scene to determine the radiance of each pixel!&lt;/p&gt; &lt;p&gt; To determine the direct lighting of the scene, I sum over all of the light sources in the scene. From each light, I take sample rays and compute the incoming radiance from those directions. Then I convert the incoming radiance to outgoing radiance using the Bidirectional Scattering Distribution Function (BSDF) of the surface. &lt;/p&gt;

        &lt;img src=&quot;../images/p3/part3/dragon_l1.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Dragon with 1 sample per Light Source &lt;br /&gt; The image is very noisy!&lt;/figcaption&gt;
        &lt;img src=&quot;../images/p3/part3/dragon_l16.png&quot; /&gt;
        &lt;figcaption align=&quot;middle&quot;&gt;Dragon with 16 samples per Light Source &lt;br /&gt; Notice the shadows on the dragon&#39;s neck and tail are much more distinct! &lt;/figcaption&gt; 
&lt;br /&gt;

 &lt;h2 align=&quot;middle&quot;&gt;Part 4: Indirect Illumination&lt;/h2&gt;
&lt;p&gt;Now, instead of just considering light rays directly from the light to the surface, I let the rays &quot;bounce&quot;. This lets me detect radiance that is coming not only directly from the light, but from light bouncing off the other objects! &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part4/bunny_indirect.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with only Indirect Lighting ... &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/bunny_direct.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with only Direct Lighting &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/CBbunny_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Cornell Box Bunny with Global Lighting (Indirect and Direct) &lt;br /&gt; Notice the colors visible in the shadows!&lt;/figcaption&gt; 
            &lt;br /&gt; 
&lt;p&gt; To accomplish this, first I take a sample from the surface BSDF at the point where the ray hits. Using the illumination from that sample, I determine whether to stop there or let the ray bounce again. I can&#39;t just let the rays bounce forever (too expensive!), but fortunately I can achieve a pretty realistic result using &quot;Russian Roulette&quot; to decide how whether or not to terminate the ray. I flip a biased coin with probability proportional to illumination of returning true. If the illumination is already very low, I am more likely to terminate the ray. If it is very high, I am more likely to let it keep bouncing. This way even though I am sampling &quot;randomly&quot;, I am making sure the brightest rays are being represented. &lt;/p&gt;
&lt;p&gt; If the coinflip returns true, I recursively trace the ray again, offsetting it&#39;s origin slightly and sending it off in the direction of the incoming radiance converted to world coordinates. The light from the ray is accumulated in the scene until the russian roulette terminates it. &lt;/p&gt; 
&lt;h3&gt; Global Illumination at varying Sample Rates &lt;/h3&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 1 Sample per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s4.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 4 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s16.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 16 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s64.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 64 Samples per Pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres_s1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt;Lambertian Spheres with 1024 Samples per Pixel &lt;/figcaption&gt;
            &lt;br /&gt;
&lt;p&gt; As you can see, it takes a lot of samples per pixel to completely eliminate the noise in the image! Images with only one sample per pixel take seconds to render, while images with 1024 samples take hours! &lt;/p&gt;
&lt;h3&gt; Varying Camera Ray Depths &lt;/h3&gt;
&lt;p&gt; This is skipping ahead a little, but once I add the glass and metallic materials, you can clearly see the effects of increasing the max ray depth. &lt;/p&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m1.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 1. &lt;br /&gt; Rays enter the glass sphere but they never leave! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m2.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 2. &lt;br /&gt; Now the rays bounce inside the glass! &lt;br /&gt; You can tell they are still trapped inside because the mirror ball still shows a dark glass sphere. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m3.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 3. &lt;br /&gt; Finally the rays are making it outside of the glass. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m4.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 4. &lt;br /&gt; Now the rays are showing up on the right wall too! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part4/spheres2_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 100. &lt;/figcaption&gt; &lt;br /&gt; 


&lt;h2 align=&quot;middle&quot;&gt;Part 5: Materials&lt;/h2&gt;
&lt;img src=&quot;../images/p3/part5/spheres2_100_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Mirror and Glass Ball &lt;/figcaption&gt;
&lt;p&gt; Remember the Bidirectional Scattering Distribution Functions I mentioned earlier? They take convert the incoming radiance of a ray to outgoing radiance. Different materials have different BSDFs, since light reacts differently to them. I implemented glass and mirror BSDFs to generate the image you see above.&lt;/p&gt;  
&lt;h3&gt; Mirror and Glass BSDF &lt;/h3&gt;
&lt;p&gt; The mirror ball (left) requires a mirror BSDF. This was relatively simple to implement, since the ray just reflects off of the mirror. The glass ball (right) is much more complicated. Sometimes light reflects off glass, and sometimes it refracts inside! To simulate this, I used another biased coin flip based off of the Schlick&#39;s Approximation. If the coin returned true, I reflected the incoming radiance as I did with the mirror BSDF, except with the Schlick coefficient as the probability density function (for mirror the probability density function was 1). Otherwise, I refracted the incoming radiance. &lt;/p&gt;

            &lt;img src=&quot;../images/p3/part5/spheres2_1_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 1. &lt;br /&gt; Rays enter the glass sphere but they never leave! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_2_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 2. &lt;br /&gt; Now the rays bounce inside the glass! &lt;br /&gt; You can tell they are still trapped inside because the mirror ball still shows a dark glass sphere. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_3_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 3. &lt;br /&gt; Finally the rays are making it outside of the glass. &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_4_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 4. &lt;br /&gt; Now the rays are showing up on the right wall, and bouncing back onto the glass! &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_100_1024.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Spheres with max_ray_depth = 100. &lt;/figcaption&gt; &lt;br /&gt; 

&lt;h3&gt; Increasing Samples per Pixel &lt;/h3&gt;
&lt;figcaption align=&quot;middle&quot;&gt; One Sample per Light and max_ray_depth = 100 &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s1_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 1 sample per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s4_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 4 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s16_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 16 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s64_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 64 samples per pixel &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/part5/spheres2_s1024_m100.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; 1024 samples per pixel &lt;/figcaption&gt; &lt;br /&gt; 



&lt;h2 align=&quot;middle&quot;&gt; Sick Renders! &lt;/h2&gt;
&lt;figcaption align=&quot;middle&quot;&gt; Here are my favorite renders! All with 1024 Samples/Pixel.&lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/fun/dragon_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Mirror Dragon &lt;/figcaption&gt;
            &lt;img src=&quot;../images/p3/fun/CBgems_hd.png&quot; /&gt;
            &lt;figcaption align=&quot;middle&quot;&gt; Gems &lt;/figcaption&gt;




&lt;/li&gt;&lt;/li&gt;&lt;/div&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 31 Mar 2016 00:00:00 -0700</pubDate>
        <link>http://kadie16.github.io/raytracer/</link>
        <guid isPermaLink="true">http://kadie16.github.io/raytracer/</guid>
        
        
        <category>featured</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>Mesh Editing and GLSL</title>
        <description>&lt;div&gt;
        &lt;p&gt;In this project, I first learned to tesselate bezier patches to generate triangular meshes from bezier curves. Then, I implemented the ability to modify those meshes by flipping and splitting edges, or &quot;upsampling&quot; to increase the amount of triangle faces that represent the mesh. Finally, I wrote some cool GLSL shaders. My favorite was the reflection shader, which maps each pixel on the surface to a place on a reflection texture. &lt;/p&gt;

    &lt;h2 align=&quot;middle&quot;&gt;Part 1: Fun with Bezier Patches&lt;/h2&gt;
        &lt;p&gt;The first thing I needed to do was generate a triangle mesh from an input Bezier Surface. A Bezier surface is specified by control points (16, in this case). The control points specify curves, and the convex hull of the control points contains a surface. That surface is the mesh I am trying to represent. &lt;/p&gt;
        &lt;p&gt;In order to represent the surface as a triangular mesh, I need to tesselate the Bezier surface specified by the control points into triangles. I accomplished this using the Bernstein Polynomials.I wrote some helper functions to help me evaluate the Bernstein Polynomials, used them to evaluate each control point. Once I determined which triangles I needed, I added them to a halfedge data structure to represent the mesh. &lt;/p&gt;
                    &lt;img src=&quot;../images/part1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;My Rendering of the teapot.bez Mesh&lt;/figcaption&gt;
        &lt;h2 align=&quot;middle&quot;&gt;Part 2: Average normals for half-edge meshes&lt;/h2&gt;
        &lt;p&gt;Next, I computed per vertex normals that were an average of the normals of their faces. This results in a nicer, smoother shading effect than when normal vectors are just computer per face. To accomplish this, I traversed the faces that the vertex belonged to in the half edge data structure, computing the normal for each face along the way. First I got the half edge that the vertex was attached to. Then, I saved it as h_original so I could keep track of where I started. Then I got the halfedge&#39;s twin and next half edge, and computed the vectors those half edges represented. I computed the normal of that face by crossing those two vectors before advancing to the next face. Once I reached h_original again, indicating I had tranversed all of the faces of that vertex, I returned the unit vector of the sum of all of the normal vectors as the normal vector for that particular vertex. &lt;/p&gt;
                    &lt;img src=&quot;../images/part2/facenormals.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Each vertex is shaded uniformly according to the normal vector of one face.&lt;/figcaption&gt;

                    &lt;img src=&quot;../images/part2/avgnormals.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;This shows the teapot shaded according to per vertex normal vectors.&lt;/figcaption&gt;

                    &lt;img src=&quot;../images/part2/bug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Initially I was calculating the vectors for the first face incorectly, which gave this cool swirly effect. &lt;/figcaption&gt;

         &lt;h2 align=&quot;middle&quot;&gt;Part 3: Edge Flip&lt;/h2&gt;
        &lt;p&gt;Next, I implemented the abillity to flip edges in the mesh. As I mentioned before, the mesh is represented as a half edge data structure. A half edge data structure is mostly a few key components: vertices, halfedges, edges, and faces. Vertices, edges, and faces each keep track of a single half edge. Halfedges each keep track of a twin halfedge (the one on the other side of the edge), a next halfedge (the next one on the face), an edge (made up of the half edge and its&#39; twin), and a face. &lt;/p&gt;
        &lt;p&gt;In order to accomplish the edge flip, I just needed to reassign the pointers of all of the elements involving the edge before and after the flip.&lt;/p&gt;
                    &lt;img src=&quot;../images/part3/bug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bug: Every edge I tried to flip poked a hole in my mesh!&lt;/figcaption&gt;
        &lt;p&gt; Initially I was only reassigning pointers for the elements on the &lt;i&gt;inside&lt;/i&gt; of the two faces between the edge I was trying to flip. Can you imagine why this would poke holes in my mesh? I was neglecting to reassign pointers for the halfedges on the outside of the borders of the faces. So while the edge may have been flipped correctly, the halfedges in the rest of the mesh weren&#39;t informed of the changes, and lost track of (could no longer traverse) that part of the mesh!&lt;/p&gt; 
        &lt;p&gt; Once I assigned the pointers to the outside half edges too, I got the desired result.  &lt;/p&gt;
                    &lt;img src=&quot;../images/part3/preflip.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;A Torus mesh before any edges are flipped.&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part3/postflip.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Torus mesh after flipping some edges.&lt;/figcaption&gt;
         &lt;h2 align=&quot;middle&quot;&gt;Part 4: Edge Split&lt;/h2&gt;
        &lt;p&gt;Next, I implemented the ability to split edges of the mesh. This was somewhat similar to flipping edges in that it involved a lot of reassigning of pointers. However, splitting became a little more complicated because it involved adding some new edges, faces, and a new vertex to the mesh. &lt;/p&gt;
                    &lt;img src=&quot;../images/part4/bug2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bug: Whenever I attempted an edge split, I punched a huge dent in my teapot! This shows a bunch of split-attempts near eachother, resulting in one giant heart dent. &lt;/figcaption&gt;
        &lt;p&gt; This initial error was caused by an error calculating the position of my new vertex. In fact, I wasn&#39;t calculating the position of the vertex at all. So everytime I split an edge, I was adding a new vertex at the origin. That&#39;s why when I split, all of these dents were getting punched toward the same spot! &lt;/p&gt; 
        &lt;p&gt; Once I updated the position of the new vertex to be the midpoint between the other corner vertices, I achieved the desired result. &lt;/p&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part4/pre_split.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bean mesh before edges are split.&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part4/post_split.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bean mesh with some split edges in the middle.&lt;/figcaption&gt;

        &lt;h2 align=&quot;middle&quot;&gt;Part 5: Upsampling via Loop Subdivision&lt;/h2&gt;
        &lt;p&gt;Upsampling was the most challenging part of the project for me. There were quite a few issues to be debugged, but at a high level the approach is pretty straight forward.&lt;/p&gt;
        &lt;li&gt; 
            First I marked all of the existing vertices as &quot;old&quot;, indicating they were part of the mesh before it was upsampled. 
            &lt;li&gt; 
            Next I calculated new positions for all of the old vertices according to the vertex subdivision rule.
        &lt;/li&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;n = vertex degree&lt;/pre&gt;&lt;/p&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;u = 3/(8*n)&lt;/pre&gt;&lt;/p&gt;
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;(1 - 3/8) * original_position + u * neighbor_position_sum&lt;/pre&gt;&lt;/p&gt;
        &lt;li&gt; 
            Then I calculated positions for all of the new (to be added) vertices according to: 
        &lt;/li&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;3/8 * (A + B) + 1/8 * (C + D)&lt;/pre&gt;&lt;/p&gt;
            &lt;p&gt; where A and B are the vertex positions on the edge that the new vertex will lie, and C and D are the vertex positons on the adjacent edges of the faces on either side of the edge that the new vertex will lie. &lt;/p&gt;
        &lt;li&gt; Next, I split every &quot;old&quot; or prexisting edge in the mesh.
        &lt;li&gt;Finally, I flipped any edge that connected a new vertex and an existing vertex, and then updated all of the vertex positions. 
        &lt;/li&gt; 
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Teapot mesh before upsample&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_teapot.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Teapot mesh after upsample&lt;/figcaption&gt;

        &lt;p&gt; Some times a little pre processing before upsampling a mesh can help significantly. A good example of this is the simple cube mesh. The way the edges are oriented can cause some irregularity when then mesh is upsampled. The upsampling behavior can be improved significantly if the edges that cross the faces of the cube are split prior to upsampling. This gives a more symmetrical mesh, which helps the upsampling give a symmetrical result. &lt;/p&gt;
        &lt;p&gt;Here you can see a side by side comparison of the upsampling of two cubes. The &lt;b&gt; left cube &lt;/b&gt; is the original mesh being upsampled. The &lt;b&gt; right cube &lt;/b&gt; had each of the face crossing edges split before it is upsampled.  

                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube mesh before upsample&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/split_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cube mesh with each face of cube split before upsample&lt;/figcaption&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube after upsample (1X). Note the irregularity&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_split_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Pre-split cube mesh after upsample (1X).&lt;/figcaption&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_cube_2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube after multiple upsamples. The irregularity has propagated.&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_split_cube_3.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Pre-split cube mesh after multiple upsamples. Looks like a nice smooth cube!&lt;/figcaption&gt;
         &lt;h2 align=&quot;middle&quot;&gt;Part 6: Fun with Shaders&lt;/h2&gt;
        &lt;p&gt;Part 6 was my favorite! I wrote GLSL shaders which specify how OpenGL should light the scene. I implemented a simple Blinn Phong shader, and a really cool reflection environment map shader. &lt;/p&gt;
                    &lt;img src=&quot;../images/part6/phongcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Pink Phong Shader&lt;/figcaption&gt;
        &lt;p&gt;For the Phong shader, I followed the following procedue:
            &lt;li&gt; Calculate the light vector, l 
            &lt;li&gt; Calculate the vector to my &quot;eye position&quot;, v
            &lt;li&gt; Normalize the sum of the vectors l and v
            &lt;li&gt; Return a linear combination of the ambient, diffuse, and specular lights: 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;LightVec = ambient_light + diffuse_light*max(dot(normal_vector, l), 0) + specular_light*(max(dot(n,normalize(l+v)), 0)^shininess_factor&lt;/pre&gt;&lt;/p&gt;
            
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part6/defaultcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Default Shader&lt;/figcaption&gt;
             
                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part6/greyphongcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Grey Phong Shader&lt;/figcaption&gt;
                &lt;br /&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part6/defaultbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Default Shader&lt;/figcaption&gt;
                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part6/phongbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Phong Shader&lt;/figcaption&gt;

        &lt;p&gt; I liked the environment map reflection shader the best. This is what I did to accomplish this effect: 
            &lt;li&gt; Calculate the vector to my &quot;eye position&quot;, v
            &lt;li&gt; Calculate the reflection vector using v and the normal vector of the vertex 
            &lt;li&gt; Convert the reflection vector into polar coordinates
            &lt;li&gt; Convert the polar coordinates into uv coordinates 
            &lt;li&gt; Retrieve the colors for the vertex from the environment map using the uv coordinates. 
         
                    &lt;img src=&quot;../images/part6/reflectcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Reflection Map Shader&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part6/reflectbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Reflection Map Shader&lt;/figcaption&gt;




&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/li&gt;&lt;/li&gt;&lt;/td&gt;&lt;/td&gt;&lt;/div&gt;
</description>
        <pubDate>Tue, 01 Mar 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/mesh-editing-and-glsl/</link>
        <guid isPermaLink="true">http://kadie16.github.io/mesh-editing-and-glsl/</guid>
        
        
        <category>projects</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>Mesh Editing and GLSL</title>
        <description>&lt;div&gt;
        &lt;p&gt;In this project, I first learned to tesselate bezier patches to generate triangular meshes from bezier curves. Then, I implemented the ability to modify those meshes by flipping and splitting edges, or &quot;upsampling&quot; to increase the amount of triangle faces that represent the mesh. Finally, I wrote some cool GLSL shaders. My favorite was the reflection shader, which maps each pixel on the surface to a place on a reflection texture. &lt;/p&gt;

    &lt;h2 align=&quot;middle&quot;&gt;Part 1: Fun with Bezier Patches&lt;/h2&gt;
        &lt;p&gt;The first thing I needed to do was generate a triangle mesh from an input Bezier Surface. A Bezier surface is specified by control points (16, in this case). The control points specify curves, and the convex hull of the control points contains a surface. That surface is the mesh I am trying to represent. &lt;/p&gt;
        &lt;p&gt;In order to represent the surface as a triangular mesh, I need to tesselate the Bezier surface specified by the control points into triangles. I accomplished this using the Bernstein Polynomials.I wrote some helper functions to help me evaluate the Bernstein Polynomials, used them to evaluate each control point. Once I determined which triangles I needed, I added them to a halfedge data structure to represent the mesh. &lt;/p&gt;
                    &lt;img src=&quot;../images/part1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;My Rendering of the teapot.bez Mesh&lt;/figcaption&gt;
        &lt;h2 align=&quot;middle&quot;&gt;Part 2: Average normals for half-edge meshes&lt;/h2&gt;
        &lt;p&gt;Next, I computed per vertex normals that were an average of the normals of their faces. This results in a nicer, smoother shading effect than when normal vectors are just computer per face. To accomplish this, I traversed the faces that the vertex belonged to in the half edge data structure, computing the normal for each face along the way. First I got the half edge that the vertex was attached to. Then, I saved it as h_original so I could keep track of where I started. Then I got the halfedge&#39;s twin and next half edge, and computed the vectors those half edges represented. I computed the normal of that face by crossing those two vectors before advancing to the next face. Once I reached h_original again, indicating I had tranversed all of the faces of that vertex, I returned the unit vector of the sum of all of the normal vectors as the normal vector for that particular vertex. &lt;/p&gt;
                    &lt;img src=&quot;../images/part2/facenormals.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Each vertex is shaded uniformly according to the normal vector of one face.&lt;/figcaption&gt;

                    &lt;img src=&quot;../images/part2/avgnormals.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;This shows the teapot shaded according to per vertex normal vectors.&lt;/figcaption&gt;

                    &lt;img src=&quot;../images/part2/bug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Initially I was calculating the vectors for the first face incorectly, which gave this cool swirly effect. &lt;/figcaption&gt;

         &lt;h2 align=&quot;middle&quot;&gt;Part 3: Edge Flip&lt;/h2&gt;
        &lt;p&gt;Next, I implemented the abillity to flip edges in the mesh. As I mentioned before, the mesh is represented as a half edge data structure. A half edge data structure is mostly a few key components: vertices, halfedges, edges, and faces. Vertices, edges, and faces each keep track of a single half edge. Halfedges each keep track of a twin halfedge (the one on the other side of the edge), a next halfedge (the next one on the face), an edge (made up of the half edge and its&#39; twin), and a face. &lt;/p&gt;
        &lt;p&gt;In order to accomplish the edge flip, I just needed to reassign the pointers of all of the elements involving the edge before and after the flip.&lt;/p&gt;
                    &lt;img src=&quot;../images/part3/bug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bug: Every edge I tried to flip poked a hole in my mesh!&lt;/figcaption&gt;
        &lt;p&gt; Initially I was only reassigning pointers for the elements on the &lt;i&gt;inside&lt;/i&gt; of the two faces between the edge I was trying to flip. Can you imagine why this would poke holes in my mesh? I was neglecting to reassign pointers for the halfedges on the outside of the borders of the faces. So while the edge may have been flipped correctly, the halfedges in the rest of the mesh weren&#39;t informed of the changes, and lost track of (could no longer traverse) that part of the mesh!&lt;/p&gt; 
        &lt;p&gt; Once I assigned the pointers to the outside half edges too, I got the desired result.  &lt;/p&gt;
                    &lt;img src=&quot;../images/part3/preflip.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;A Torus mesh before any edges are flipped.&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part3/postflip.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Torus mesh after flipping some edges.&lt;/figcaption&gt;
         &lt;h2 align=&quot;middle&quot;&gt;Part 4: Edge Split&lt;/h2&gt;
        &lt;p&gt;Next, I implemented the ability to split edges of the mesh. This was somewhat similar to flipping edges in that it involved a lot of reassigning of pointers. However, splitting became a little more complicated because it involved adding some new edges, faces, and a new vertex to the mesh. &lt;/p&gt;
                    &lt;img src=&quot;../images/part4/bug2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bug: Whenever I attempted an edge split, I punched a huge dent in my teapot! This shows a bunch of split-attempts near eachother, resulting in one giant heart dent. &lt;/figcaption&gt;
        &lt;p&gt; This initial error was caused by an error calculating the position of my new vertex. In fact, I wasn&#39;t calculating the position of the vertex at all. So everytime I split an edge, I was adding a new vertex at the origin. That&#39;s why when I split, all of these dents were getting punched toward the same spot! &lt;/p&gt; 
        &lt;p&gt; Once I updated the position of the new vertex to be the midpoint between the other corner vertices, I achieved the desired result. &lt;/p&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part4/pre_split.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bean mesh before edges are split.&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part4/post_split.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Bean mesh with some split edges in the middle.&lt;/figcaption&gt;

        &lt;h2 align=&quot;middle&quot;&gt;Part 5: Upsampling via Loop Subdivision&lt;/h2&gt;
        &lt;p&gt;Upsampling was the most challenging part of the project for me. There were quite a few issues to be debugged, but at a high level the approach is pretty straight forward.&lt;/p&gt;
        &lt;li&gt; 
            First I marked all of the existing vertices as &quot;old&quot;, indicating they were part of the mesh before it was upsampled. 
            &lt;li&gt; 
            Next I calculated new positions for all of the old vertices according to the vertex subdivision rule.
        &lt;/li&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;n = vertex degree&lt;/pre&gt;&lt;/p&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;u = 3/(8*n)&lt;/pre&gt;&lt;/p&gt;
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;(1 - 3/8) * original_position + u * neighbor_position_sum&lt;/pre&gt;&lt;/p&gt;
        &lt;li&gt; 
            Then I calculated positions for all of the new (to be added) vertices according to: 
        &lt;/li&gt; 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;3/8 * (A + B) + 1/8 * (C + D)&lt;/pre&gt;&lt;/p&gt;
            &lt;p&gt; where A and B are the vertex positions on the edge that the new vertex will lie, and C and D are the vertex positons on the adjacent edges of the faces on either side of the edge that the new vertex will lie. &lt;/p&gt;
        &lt;li&gt; Next, I split every &quot;old&quot; or prexisting edge in the mesh.
        &lt;li&gt;Finally, I flipped any edge that connected a new vertex and an existing vertex, and then updated all of the vertex positions. 
        &lt;/li&gt; 
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Teapot mesh before upsample&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_teapot.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Teapot mesh after upsample&lt;/figcaption&gt;

        &lt;p&gt; Some times a little pre processing before upsampling a mesh can help significantly. A good example of this is the simple cube mesh. The way the edges are oriented can cause some irregularity when then mesh is upsampled. The upsampling behavior can be improved significantly if the edges that cross the faces of the cube are split prior to upsampling. This gives a more symmetrical mesh, which helps the upsampling give a symmetrical result. &lt;/p&gt;
        &lt;p&gt;Here you can see a side by side comparison of the upsampling of two cubes. The &lt;b&gt; left cube &lt;/b&gt; is the original mesh being upsampled. The &lt;b&gt; right cube &lt;/b&gt; had each of the face crossing edges split before it is upsampled.  

                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube mesh before upsample&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/split_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cube mesh with each face of cube split before upsample&lt;/figcaption&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube after upsample (1X). Note the irregularity&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_split_cube.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Pre-split cube mesh after upsample (1X).&lt;/figcaption&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_cube_2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Regular cube after multiple upsamples. The irregularity has propagated.&lt;/figcaption&gt;

                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part5/up_split_cube_3.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Pre-split cube mesh after multiple upsamples. Looks like a nice smooth cube!&lt;/figcaption&gt;
         &lt;h2 align=&quot;middle&quot;&gt;Part 6: Fun with Shaders&lt;/h2&gt;
        &lt;p&gt;Part 6 was my favorite! I wrote GLSL shaders which specify how OpenGL should light the scene. I implemented a simple Blinn Phong shader, and a really cool reflection environment map shader. &lt;/p&gt;
                    &lt;img src=&quot;../images/part6/phongcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Pink Phong Shader&lt;/figcaption&gt;
        &lt;p&gt;For the Phong shader, I followed the following procedue:
            &lt;li&gt; Calculate the light vector, l 
            &lt;li&gt; Calculate the vector to my &quot;eye position&quot;, v
            &lt;li&gt; Normalize the sum of the vectors l and v
            &lt;li&gt; Return a linear combination of the ambient, diffuse, and specular lights: 
            &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;LightVec = ambient_light + diffuse_light*max(dot(normal_vector, l), 0) + specular_light*(max(dot(n,normalize(l+v)), 0)^shininess_factor&lt;/pre&gt;&lt;/p&gt;
            
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part6/defaultcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Default Shader&lt;/figcaption&gt;
             
                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part6/greyphongcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Grey Phong Shader&lt;/figcaption&gt;
                &lt;br /&gt;
                    &lt;td align=&quot;left&quot;&gt;
                    &lt;img src=&quot;../images/part6/defaultbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Default Shader&lt;/figcaption&gt;
                    &lt;td align=&quot;right&quot;&gt;
                    &lt;img src=&quot;../images/part6/phongbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Phong Shader&lt;/figcaption&gt;

        &lt;p&gt; I liked the environment map reflection shader the best. This is what I did to accomplish this effect: 
            &lt;li&gt; Calculate the vector to my &quot;eye position&quot;, v
            &lt;li&gt; Calculate the reflection vector using v and the normal vector of the vertex 
            &lt;li&gt; Convert the reflection vector into polar coordinates
            &lt;li&gt; Convert the polar coordinates into uv coordinates 
            &lt;li&gt; Retrieve the colors for the vertex from the environment map using the uv coordinates. 
         
                    &lt;img src=&quot;../images/part6/reflectcow.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Cow With Reflection Map Shader&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part6/reflectbug.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Beetle With Reflection Map Shader&lt;/figcaption&gt;




&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/li&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/li&gt;&lt;/li&gt;&lt;/td&gt;&lt;/td&gt;&lt;/div&gt;
</description>
        <pubDate>Tue, 01 Mar 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/mesh-editing-and-glsl/</link>
        <guid isPermaLink="true">http://kadie16.github.io/mesh-editing-and-glsl/</guid>
        
        
        <category>featured</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>My First Maya Project</title>
        <description>&lt;p&gt;This semester I am taking a decal called UCBUGG, which stands for UC Berkeley Undergraduate Graphics Group. We are learning how to make an animated short using maya! This is my first maya project. He is a little robot. I dont have a name for him but I think he is pretty cute. Some people said he looks like a penguin. My former supervisor Like thinks he looks like a bumble bee :)&lt;/p&gt;

&lt;center&gt;&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/plv-S4iDUYM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;p&gt;I used &lt;a href=&quot;http://ucbugg-labs.wikispaces.com/Introduction+to+Maya&quot;&gt;this tutorial&lt;/a&gt; to make this little guy. It was super helpful! The whole website has a lot of great tutorials if you are just starting out with Maya like I am.&lt;/p&gt;

</description>
        <pubDate>Mon, 01 Feb 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/maya-robot/</link>
        <guid isPermaLink="true">http://kadie16.github.io/maya-robot/</guid>
        
        <category>Java</category>
        
        
        <category>projects</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>My First Maya Project</title>
        <description>&lt;p&gt;This semester I am taking a decal called UCBUGG, which stands for UC Berkeley Undergraduate Graphics Group. We are learning how to make an animated short using maya! This is my first maya project. He is a little robot. I dont have a name for him but I think he is pretty cute. Some people said he looks like a penguin. My former supervisor Like thinks he looks like a bumble bee :)&lt;/p&gt;

&lt;center&gt;&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/plv-S4iDUYM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;p&gt;I used &lt;a href=&quot;http://ucbugg-labs.wikispaces.com/Introduction+to+Maya&quot;&gt;this tutorial&lt;/a&gt; to make this little guy. It was super helpful! The whole website has a lot of great tutorials if you are just starting out with Maya like I am.&lt;/p&gt;

</description>
        <pubDate>Mon, 01 Feb 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/maya-robot/</link>
        <guid isPermaLink="true">http://kadie16.github.io/maya-robot/</guid>
        
        <category>Java</category>
        
        
        <category>featured</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>Rasterizester Project</title>
        <description>&lt;h2 align=&quot;middle&quot;&gt;Part 1: Rasterizing Lines&lt;/h2&gt;
&lt;p&gt;I used Bresenham&#39;s algorithm to rasterize lines. In order to rasterize the line, the algorithm needs to decide which pixels lie closest to it. The line will not directly intersect the center of every pixel, so it must be decided which pixels it intersects the most. To do this, it steps through the line, incrementing the x coordinate by one if the overall change in x is greater than the overall change in y, or incrementing the y coordinate if the opposite is true. For this description I will assume we are rasterizing a line dx &amp;gt; dy. &lt;/p&gt;
&lt;p&gt; Then the algorithm needs to make a decision about which pixel is closest to the next point on the line. It uses a decision parameter pk. If pk is less than zero, it plots the point (x, y) and increments pk by 2*dy. Otherwise, it plots the point (x, y + 1) or (x, y - 1) depending on if the slope is positive or negative, respectively. It that case it increments pk by 2*dy - 2*dx and continues stepping through the line. The multiplications by two make the algorithm only use integer calculations, which makes it more efficient. 

                    &lt;img src=&quot;../images/part_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Result Using Bresenham&#39;s Line Rasterizing Algorithm &lt;/figcaption&gt;&lt;br /&gt;

&lt;h2 align=&quot;middle&quot;&gt;Part 2:Rasterizing Single-Color Triangles&lt;/h2&gt;
&lt;p&gt;I ended up using two different methods to rasterize triangles. In the first method, I was breaking the triangles into &quot;top flat&quot; and &quot;bottom flat&quot; cases, and then stepping through the triangle and rendering it line by line. After part 5, I ended up switching to using barycentric coordinates to rasterize them. I needed to calculate barycentric coordinates to retrieve the correct color for each pixel, so it made sense to rasterize each pixel individually rather than rasterize a line at a time. Using barycentric coordinates to rasterize triangles also gave a cleaner result. 
					&lt;img src=&quot;../images/part_2_1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Original Method: Note the lines extending off the claw and tail.&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part_2_2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Barycentric Method&lt;/figcaption&gt;
            &lt;p&gt;In the first method I used to rasterize triangles, I broke them into three cases: 
            &lt;ul&gt;
            &lt;li&gt;Bottom Flat Triangles&lt;/li&gt;
            &lt;li&gt;Top Flat Triangles&lt;/li&gt;
            &lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;
        For the bottom flat triangles, I started by using my rasterize_line function to rasterize the bottom (flat) edge of the triangle, which was just the line between the two vertices with the lowest y coordinate. From there I calculated the inverse slope of each non-flat side of the triangle, and added that to the respective x coordinate. Then I incremented the y coordinate and plotted a line between the current x values and the y value.&lt;/p&gt;
        &lt;p&gt;Incrementing the x values and plotting the line: &lt;/p&gt;
        &lt;p align=&quot;middle&quot;&gt;
        &lt;pre align=&quot;middle&quot;&gt;current_x1 = current_x1 + m1^-1&lt;/pre&gt;
        &lt;pre align=&quot;middle&quot;&gt;current_x2 = current_x2 + m2^-1&lt;/pre&gt;
        &lt;pre align=&quot;middle&quot;&gt;rasterize_line(current_x1, y, current_x2, y)&lt;/pre&gt;&lt;/p&gt;
        &lt;p&gt; Where current_x1 is initialized to the x value of one of the bottom corners and current_x2 is the x value of the other bottom corner. Adding the inverse slope steps the x_values up along the edges of the triangle towards the top vertex. The subroutine to render top flat triangles was done similarly&lt;/p&gt; 
        &lt;p&gt; For the &quot;other&quot; case, I would simply divide the triangle into two seperate triangles: a bottom flat triangle and a top flat triangle. I generated a &quot;fourth&quot; vertex that was directly across from the &quot;middle&quot; vertex, the one with the middle y coordinate. &lt;/p&gt; 
        &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt; x4 = x0 + ((y1 - y0)/(y2 - y0) * (x2 - x0))&lt;/pre&gt;&lt;/p&gt;
        &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;y4 = y1&lt;/pre&gt;&lt;/p&gt;
        &lt;p&gt; Where coordinates are sorted according to ascending y coordinates (so (x0, y0) has the smallest y coordinate and (x2, y2) has the highest y coordinate). Using this new fourth coordinate, I could use my prewritten subroutines to render the original triangle as two seperate triangles, one that was flat on top and the other which was flat on bottom. &lt;/p&gt; 
&lt;img src=&quot;../images/part2.png&quot; /&gt;
&lt;center&gt;Final Result: Rendered Triangles&lt;/center&gt;

&lt;h2 align=&quot;middle&quot;&gt; Part 3: Antialiasing triangles&lt;/h2&gt;
&lt;br /&gt;
&lt;p&gt; Part three was the most challenging for me! First I initialized the super sample buffer as a vector of unsigned chars, simiar to the frame rate buffer, except scaled up based on the sample rate. 
&lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;total_sample_pts = width * height * sample_rate
&lt;br /&gt; 
superframebuffer.resize(total_sample_pts * 4)&lt;/pre&gt;&lt;/p&gt;

&lt;center&gt;
                    &lt;img src=&quot;../images/part_3_off_by_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 4. Image colors are off and image is repeating. The colors are off due to the indexing in red shown above. It should be: 
                    &lt;pre&gt; int index = (k*dimension) + &lt;font color=&quot;red&quot;&gt; &lt;font color=&quot;blue&quot;&gt;4 *&lt;/font&gt;(i + j)&lt;/font&gt;&lt;/pre&gt; Due to the supersamplebuffer storing rgba values for each pixel. &lt;/figcaption&gt;&lt;/center&gt;&lt;br /&gt;

&lt;p&gt; The image is being repeated due to another indexing error when the pixels are drawn into the supersamplebuffer. Once the index was corrected, I got a new result:&lt;/p&gt;


                    &lt;img src=&quot;../images/part_3_indexbug.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;&lt;br /&gt;Sample_Rate = 4. Image is scaled up larger than it should be and is lighter due to super sample pixel being blended with blank pixels.  &lt;/figcaption&gt;

&lt;p&gt; The super sample pixel is:
        &lt;p&gt;&lt;pre align=&quot;middle&quot;&gt; 1/sqrt(sample_rate) &lt;/pre&gt;
            the size of a frame buffer pixel, so it&#39;s color blended with:
            &lt;pre align=&quot;middle&quot;&gt; (sqrt(sample_rate) - 1)/sqrt(sample_rate) &lt;/pre&gt; white pixels, giving the lighter, less opaque appearance.&lt;/p&gt;
        &lt;p&gt; I went through countless other iterations of indexing errors, many of which I can&#39;t recreate now. I ended up changing my resolve method to keep track of the x and y coordinate of the current frame buffer pixel. However, just now when trying to recreate another bug, I got my original resolve code to work. Still, my supersamples were getting lighter due to the blending error I described above. &lt;/p&gt; 
        &lt;p&gt; The biggest challenge I had was understanding how to actually anti-alias. At one point I was drawing to the super sample buffer correctly, my resolve method was working mostly correctly (the size of the images stayed consistent among different sample_rates), however there was no anti-aliasing. The jaggies looked the same whether the super_sample_rate was 1 or 16. &lt;/p&gt; 
        &lt;p&gt; 
            Did I need to scale the points by sqrt(sample_rate) in my supersample_point method? Did I need to scale up the triangle according to the sample rate? Or was it both? Between this and the indexing stuff, I was really confusing myself. Thankfully, Ren patiently talked through the idea with me until I understood: I needed to scale one or the other. Either the points or the triangle, but not both.
        &lt;/p&gt;  
        &lt;p&gt;  Scaling the size of the triangle felt more intuitive to me, so that is what I went with. In my &lt;font color=&quot;blue&quot;&gt; void DrawRend::rasterize_triangle() &lt;/font&gt; method, I scaled the input coordinates (x0,y0,...,x2,y2) up by a factor of sqrt(sample_rate). Then I repaired my &lt;font color=&quot;blue&quot;&gt; DrawRend::supersample_point(x, y) &lt;/font&gt; method so that it stored the true x and y coordinates, as opposed scaling them up as I did before. This allowed (sample_rate) more pixels to be rendered inside the triangle.
        &lt;/p&gt;
        &lt;p&gt; Then the resolve method downsampled the pixels back to the resolution of the framebuffer.So colors from a sqrt(sample_rate) X sqrt(sample_rate) square of pixels in the superframebuffer were averaged, and the resulting color was assigned to one corresponding pixel in the frame buffer, giving the final result.

                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 1&lt;/figcaption&gt;
                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 4&lt;/figcaption&gt;
                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 16&lt;/figcaption&gt;



&lt;h2 align=&quot;middle&quot;&gt;Part 4: Transforms&lt;/h2&gt;
&lt;p&gt;For part 4, I implemented the transform matrices as shown in the SVG spec. Then I created a new svg file, &quot;first.svg&quot;. I grabbed one of the stars shown in another example file and pasted it in my file. Then I made four identical stars and alternated their colors. Next, I put them all in a group that would translate them closer to the center of the page. Finally, I put each star in it&#39;s own group with a rotation applied. I incremented each rotation by .25 and added enough stars to make a ring of stars!&lt;/p&gt;

&lt;img src=&quot;../images/p4_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/p_4_3.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars Showing Zoom In, Translate GUI Features&lt;br /&gt; &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/p_4_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars Showing Zoom Out, Translate GUI Features&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 5: Barycentric coordinates&lt;/h2&gt;
&lt;p&gt; Barycentric coordinates give us a way to assign each vertex in a triangle an attribute, and then linearly interpolate those attributes to assign the appropriate value for the other pixels within the triangle. &lt;/p&gt;

&lt;img src=&quot;../images/part_5_tri.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Triangle with a red vertex, blue vertex, and green vertex. The pixels between the vertices are assigned a color based on their barycentric coordinates.&lt;/figcaption&gt;&lt;br /&gt;


&lt;p&gt; In the example above, I will refer to the upper left vertex as &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;, the upper right vertex as &lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;, and the lowest vertex as &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;. All of the pixels between these vertices are assigned a color depending on where they are relative to &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;, &lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;, and &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;.
&lt;p&gt; Observe the pixels that lie on the edge between &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt; and &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;. The pixels closest to &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt; are all red, and the pixels closest to &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt; are all blue. However, the pixels right in the middle of the edge are a purple shade, since they lie directly between the red and blue vertices. The pixel that lies exactly in the center of the two vertices gets:
	&lt;pre align=&quot;center&quot;&gt;.5*color(&lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;) + .5*color(&lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;) + 0*color(&lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;)&lt;/pre&gt;
The number that is multiplied by the color of each vertex is either alpha, beta, or gamma. Each one represents the relative distance from the pixel to one of the three triangle vertices. In the above example, alpha and beta are 0.5, since the pixel is halfway between the red and blue pixels. Gamma is zero since it is relatively far away. 
To implement this in my program, I first calculate the alpha, beta, and gamma barycentric coordinates using the coordinates passed into &lt;font color=&quot;blue&quot;&gt;&amp;lt;DrawRend::render_barycentric_triangle()&lt;/font&gt;. Then, these are passed to &lt;font color=&quot;blue&quot;&gt; ColorTri::color()&lt;/font&gt;. This method then multiplies alpha by the &quot;a&quot; vertex color, beta by the &quot;b&quot; vertex color, and gamma = (1 - alpha - beta) by the &quot;c&quot; vertex co
                    &lt;img src=&quot;../images/part_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Color Wheel Result.&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 6: Pixel sampling for texture mapping&lt;/h2&gt;
&lt;h4&gt; Finding the UV Texture Coordinates &lt;/h4&gt;
&lt;p&gt; Implementing part 6 was very similar to part 5. The same way I used barycentric coordinates to interpolate the colors of three vertices of a triangle for the pixels in the triangle, I could map a coordinate inside a triangle to its corresponding uv texture coordinate. In &lt;font color=&quot;blue&quot;&gt; Color TexTri::color(Vector2D xy, Vector2D dx, Vector2D dy, SampleParams sp) &lt;/font&gt;, I multiply alpha by the &quot;a&quot; vertex&#39;s uv coordinate vector, beta by the &quot;b&quot; coordinate uv vector, and gamma by the &quot;c&quot; coordinate uv vector. &lt;/p&gt;
&lt;h4&gt; Retrieving the Color from the MipMap &lt;/h4&gt;
Once I had adjusted the uv vector, it is passed into &lt;font color=&quot;blue&quot;&gt;Texture::sample(const SampleParams &amp;amp;sp)&lt;/font&gt;, which then calls the appropriate sampling method according to the sp.psm parameter. The sampling method retreives the level 0 mip map (for this portion). The uv coordinates are between 0 and 1, so the sampling method then scales them up to match the proportions of the mipmap. 
&lt;pre align=&quot;center&quot;&gt; x = uv.x * mipmap.width &lt;br /&gt; y = uv.y * mipmap*height &lt;/pre&gt; Then, the color of the pixel is retrieved from the mipmap. 
&lt;pre align=&quot;center&quot;&gt;return MipMapPixelColor(x, y, level)&lt;/pre&gt;
&lt;h4&gt; Nearest vs Bilinear Sampling &lt;/h4&gt;
The nearest level sampling method simply returns the color of the pixel as above. The Bilinear method returns an interpolation of the four nearest uv coordinates. I used the algorithm in the book to implement bilinear sampling. The difference between nearest and bilinear sampling is the most apparent in the map sample images, which have distinct thin vertical lines through them. &lt;/p&gt;

&lt;img src=&quot;../images/part_6_nearest_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 1. &lt;br /&gt;Jaggies! Blegh!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_bilinear_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 1 &lt;br /&gt;
               		SoooOossosoSo smooth &amp;lt;3 &lt;/figcaption&gt;&lt;br /&gt;
Even with a high sample rate, the difference between the two methods is apparent on these images. &lt;br /&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_nearest_16.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 16 &lt;br /&gt;
                    	Gross! Blurry Jaggies!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_bilinear_16.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 16 &lt;br /&gt; 
                   Ridiculously good looking.&lt;/figcaption&gt;&lt;br /&gt;

&lt;br /&gt;&lt;br /&gt;
&lt;p&gt; 
	However, without these distinct lines, the effect is much less noticeable. In the campanille image, I actually prefer the nearest sampling method. The features of the image seem slightly better with the nearest sampling.  &lt;/p&gt;

&lt;img src=&quot;../images/part_6_camp_nearest.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 16 &lt;br /&gt;
                    	Looks pretty nice.&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part6_camp_bi.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 16 &lt;br /&gt; 
                    &lt;/figcaption&gt;&lt;br /&gt;
&lt;p&gt; 
	Initially when retrieving the mip map colors, I had a bug because I wasn&#39;t dividing my colors by 255. Thanks to piazza, I knew I had to divide them by 255 since the color was expecting a float between 0 and 1. However at first that was just giving me black images. Then I realized I literally had to divide it by &quot;255.&quot; to prevent a precision error. 
                    &lt;img src=&quot;../images/part_6_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Weird Colors&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Looks Like Sprinkles!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Corner Sprinkles&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 7: Level sampling with mipmaps for texture mapping&lt;/h2&gt;
&lt;p&gt;In the final part, I implemented the &quot;get level&quot; method using the math described in the textbook. This allows the texture sampling to grab different mip maps for each pixel, depending on which is most appropriate. Sometimes the effects are desirable, and other times it ends up blurring the image. &lt;/p&gt;
&lt;p&gt; In trilinear sampling, the color from the getLevel() mipMap and the color from the adjacent level are blended to give the resulting color. 
                    &lt;img src=&quot;../images/part_7_lzero_pnear.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Nearest Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_7_lzero_plin.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Linear Sampling &lt;br /&gt; &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;p&gt; I thought the above combination, Level Zero with Linear Sampling, gave the best result. It is the only one that eliminates the Jaggies in the &quot;Maleficent&quot; chrome text. &lt;/p&gt;
                    &lt;img src=&quot;../images/part_7_lnear_plin.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Linear Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_7_lnear_pnear.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Nearest Sampling&lt;/figcaption&gt;&lt;br /&gt;
&lt;p&gt; For closer images, there was visable blurring when using the trilinear sampling met
                    &lt;img src=&quot;../images/part_7_llin_maleficent.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Trilinear Sampling &lt;br /&gt; Observe the blurring in Maleficent&#39;s face. &lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part_7_llin_maleficent2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Trilinear Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part7_lnear_pnear_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Nearest Sampling&lt;br /&gt;I felt this was the best result for this image. &lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part_7_plin_lzero.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Linear Sampling &lt;br /&gt; The jaggies are less apparent in the writing, but Maleficent is a little more blurry.&lt;/figcaption&gt;&lt;br /&gt;



            



&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/rasterizester/</link>
        <guid isPermaLink="true">http://kadie16.github.io/rasterizester/</guid>
        
        
        <category>projects</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>Rasterizester Project</title>
        <description>&lt;h2 align=&quot;middle&quot;&gt;Part 1: Rasterizing Lines&lt;/h2&gt;
&lt;p&gt;I used Bresenham&#39;s algorithm to rasterize lines. In order to rasterize the line, the algorithm needs to decide which pixels lie closest to it. The line will not directly intersect the center of every pixel, so it must be decided which pixels it intersects the most. To do this, it steps through the line, incrementing the x coordinate by one if the overall change in x is greater than the overall change in y, or incrementing the y coordinate if the opposite is true. For this description I will assume we are rasterizing a line dx &amp;gt; dy. &lt;/p&gt;
&lt;p&gt; Then the algorithm needs to make a decision about which pixel is closest to the next point on the line. It uses a decision parameter pk. If pk is less than zero, it plots the point (x, y) and increments pk by 2*dy. Otherwise, it plots the point (x, y + 1) or (x, y - 1) depending on if the slope is positive or negative, respectively. It that case it increments pk by 2*dy - 2*dx and continues stepping through the line. The multiplications by two make the algorithm only use integer calculations, which makes it more efficient. 

                    &lt;img src=&quot;../images/part_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Result Using Bresenham&#39;s Line Rasterizing Algorithm &lt;/figcaption&gt;&lt;br /&gt;

&lt;h2 align=&quot;middle&quot;&gt;Part 2:Rasterizing Single-Color Triangles&lt;/h2&gt;
&lt;p&gt;I ended up using two different methods to rasterize triangles. In the first method, I was breaking the triangles into &quot;top flat&quot; and &quot;bottom flat&quot; cases, and then stepping through the triangle and rendering it line by line. After part 5, I ended up switching to using barycentric coordinates to rasterize them. I needed to calculate barycentric coordinates to retrieve the correct color for each pixel, so it made sense to rasterize each pixel individually rather than rasterize a line at a time. Using barycentric coordinates to rasterize triangles also gave a cleaner result. 
					&lt;img src=&quot;../images/part_2_1.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Original Method: Note the lines extending off the claw and tail.&lt;/figcaption&gt;
                    &lt;img src=&quot;../images/part_2_2.png&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Barycentric Method&lt;/figcaption&gt;
            &lt;p&gt;In the first method I used to rasterize triangles, I broke them into three cases: 
            &lt;ul&gt;
            &lt;li&gt;Bottom Flat Triangles&lt;/li&gt;
            &lt;li&gt;Top Flat Triangles&lt;/li&gt;
            &lt;li&gt;Other&lt;/li&gt;&lt;/ul&gt;
        For the bottom flat triangles, I started by using my rasterize_line function to rasterize the bottom (flat) edge of the triangle, which was just the line between the two vertices with the lowest y coordinate. From there I calculated the inverse slope of each non-flat side of the triangle, and added that to the respective x coordinate. Then I incremented the y coordinate and plotted a line between the current x values and the y value.&lt;/p&gt;
        &lt;p&gt;Incrementing the x values and plotting the line: &lt;/p&gt;
        &lt;p align=&quot;middle&quot;&gt;
        &lt;pre align=&quot;middle&quot;&gt;current_x1 = current_x1 + m1^-1&lt;/pre&gt;
        &lt;pre align=&quot;middle&quot;&gt;current_x2 = current_x2 + m2^-1&lt;/pre&gt;
        &lt;pre align=&quot;middle&quot;&gt;rasterize_line(current_x1, y, current_x2, y)&lt;/pre&gt;&lt;/p&gt;
        &lt;p&gt; Where current_x1 is initialized to the x value of one of the bottom corners and current_x2 is the x value of the other bottom corner. Adding the inverse slope steps the x_values up along the edges of the triangle towards the top vertex. The subroutine to render top flat triangles was done similarly&lt;/p&gt; 
        &lt;p&gt; For the &quot;other&quot; case, I would simply divide the triangle into two seperate triangles: a bottom flat triangle and a top flat triangle. I generated a &quot;fourth&quot; vertex that was directly across from the &quot;middle&quot; vertex, the one with the middle y coordinate. &lt;/p&gt; 
        &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt; x4 = x0 + ((y1 - y0)/(y2 - y0) * (x2 - x0))&lt;/pre&gt;&lt;/p&gt;
        &lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;y4 = y1&lt;/pre&gt;&lt;/p&gt;
        &lt;p&gt; Where coordinates are sorted according to ascending y coordinates (so (x0, y0) has the smallest y coordinate and (x2, y2) has the highest y coordinate). Using this new fourth coordinate, I could use my prewritten subroutines to render the original triangle as two seperate triangles, one that was flat on top and the other which was flat on bottom. &lt;/p&gt; 
&lt;img src=&quot;../images/part2.png&quot; /&gt;
&lt;center&gt;Final Result: Rendered Triangles&lt;/center&gt;

&lt;h2 align=&quot;middle&quot;&gt; Part 3: Antialiasing triangles&lt;/h2&gt;
&lt;br /&gt;
&lt;p&gt; Part three was the most challenging for me! First I initialized the super sample buffer as a vector of unsigned chars, simiar to the frame rate buffer, except scaled up based on the sample rate. 
&lt;p align=&quot;middle&quot;&gt;&lt;pre align=&quot;middle&quot;&gt;total_sample_pts = width * height * sample_rate
&lt;br /&gt; 
superframebuffer.resize(total_sample_pts * 4)&lt;/pre&gt;&lt;/p&gt;

&lt;center&gt;
                    &lt;img src=&quot;../images/part_3_off_by_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 4. Image colors are off and image is repeating. The colors are off due to the indexing in red shown above. It should be: 
                    &lt;pre&gt; int index = (k*dimension) + &lt;font color=&quot;red&quot;&gt; &lt;font color=&quot;blue&quot;&gt;4 *&lt;/font&gt;(i + j)&lt;/font&gt;&lt;/pre&gt; Due to the supersamplebuffer storing rgba values for each pixel. &lt;/figcaption&gt;&lt;/center&gt;&lt;br /&gt;

&lt;p&gt; The image is being repeated due to another indexing error when the pixels are drawn into the supersamplebuffer. Once the index was corrected, I got a new result:&lt;/p&gt;


                    &lt;img src=&quot;../images/part_3_indexbug.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;&lt;br /&gt;Sample_Rate = 4. Image is scaled up larger than it should be and is lighter due to super sample pixel being blended with blank pixels.  &lt;/figcaption&gt;

&lt;p&gt; The super sample pixel is:
        &lt;p&gt;&lt;pre align=&quot;middle&quot;&gt; 1/sqrt(sample_rate) &lt;/pre&gt;
            the size of a frame buffer pixel, so it&#39;s color blended with:
            &lt;pre align=&quot;middle&quot;&gt; (sqrt(sample_rate) - 1)/sqrt(sample_rate) &lt;/pre&gt; white pixels, giving the lighter, less opaque appearance.&lt;/p&gt;
        &lt;p&gt; I went through countless other iterations of indexing errors, many of which I can&#39;t recreate now. I ended up changing my resolve method to keep track of the x and y coordinate of the current frame buffer pixel. However, just now when trying to recreate another bug, I got my original resolve code to work. Still, my supersamples were getting lighter due to the blending error I described above. &lt;/p&gt; 
        &lt;p&gt; The biggest challenge I had was understanding how to actually anti-alias. At one point I was drawing to the super sample buffer correctly, my resolve method was working mostly correctly (the size of the images stayed consistent among different sample_rates), however there was no anti-aliasing. The jaggies looked the same whether the super_sample_rate was 1 or 16. &lt;/p&gt; 
        &lt;p&gt; 
            Did I need to scale the points by sqrt(sample_rate) in my supersample_point method? Did I need to scale up the triangle according to the sample rate? Or was it both? Between this and the indexing stuff, I was really confusing myself. Thankfully, Ren patiently talked through the idea with me until I understood: I needed to scale one or the other. Either the points or the triangle, but not both.
        &lt;/p&gt;  
        &lt;p&gt;  Scaling the size of the triangle felt more intuitive to me, so that is what I went with. In my &lt;font color=&quot;blue&quot;&gt; void DrawRend::rasterize_triangle() &lt;/font&gt; method, I scaled the input coordinates (x0,y0,...,x2,y2) up by a factor of sqrt(sample_rate). Then I repaired my &lt;font color=&quot;blue&quot;&gt; DrawRend::supersample_point(x, y) &lt;/font&gt; method so that it stored the true x and y coordinates, as opposed scaling them up as I did before. This allowed (sample_rate) more pixels to be rendered inside the triangle.
        &lt;/p&gt;
        &lt;p&gt; Then the resolve method downsampled the pixels back to the resolution of the framebuffer.So colors from a sqrt(sample_rate) X sqrt(sample_rate) square of pixels in the superframebuffer were averaged, and the resulting color was assigned to one corresponding pixel in the frame buffer, giving the final result.

                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 1&lt;/figcaption&gt;
                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 4&lt;/figcaption&gt;
                    &lt;td align=&quot;middle&quot;&gt;
                    &lt;img src=&quot;../images/part3_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt;Sample_Rate = 16&lt;/figcaption&gt;



&lt;h2 align=&quot;middle&quot;&gt;Part 4: Transforms&lt;/h2&gt;
&lt;p&gt;For part 4, I implemented the transform matrices as shown in the SVG spec. Then I created a new svg file, &quot;first.svg&quot;. I grabbed one of the stars shown in another example file and pasted it in my file. Then I made four identical stars and alternated their colors. Next, I put them all in a group that would translate them closer to the center of the page. Finally, I put each star in it&#39;s own group with a rotation applied. I incremented each rotation by .25 and added enough stars to make a ring of stars!&lt;/p&gt;

&lt;img src=&quot;../images/p4_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/p_4_3.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars Showing Zoom In, Translate GUI Features&lt;br /&gt; &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/p_4_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Ring of Stars Showing Zoom Out, Translate GUI Features&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 5: Barycentric coordinates&lt;/h2&gt;
&lt;p&gt; Barycentric coordinates give us a way to assign each vertex in a triangle an attribute, and then linearly interpolate those attributes to assign the appropriate value for the other pixels within the triangle. &lt;/p&gt;

&lt;img src=&quot;../images/part_5_tri.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Triangle with a red vertex, blue vertex, and green vertex. The pixels between the vertices are assigned a color based on their barycentric coordinates.&lt;/figcaption&gt;&lt;br /&gt;


&lt;p&gt; In the example above, I will refer to the upper left vertex as &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;, the upper right vertex as &lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;, and the lowest vertex as &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;. All of the pixels between these vertices are assigned a color depending on where they are relative to &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;, &lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;, and &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;.
&lt;p&gt; Observe the pixels that lie on the edge between &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt; and &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;. The pixels closest to &lt;font color=&quot;red&quot;&gt;R&lt;/font&gt; are all red, and the pixels closest to &lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt; are all blue. However, the pixels right in the middle of the edge are a purple shade, since they lie directly between the red and blue vertices. The pixel that lies exactly in the center of the two vertices gets:
	&lt;pre align=&quot;center&quot;&gt;.5*color(&lt;font color=&quot;blue&quot;&gt;B&lt;/font&gt;) + .5*color(&lt;font color=&quot;red&quot;&gt;R&lt;/font&gt;) + 0*color(&lt;font color=&quot;green&quot;&gt;G&lt;/font&gt;)&lt;/pre&gt;
The number that is multiplied by the color of each vertex is either alpha, beta, or gamma. Each one represents the relative distance from the pixel to one of the three triangle vertices. In the above example, alpha and beta are 0.5, since the pixel is halfway between the red and blue pixels. Gamma is zero since it is relatively far away. 
To implement this in my program, I first calculate the alpha, beta, and gamma barycentric coordinates using the coordinates passed into &lt;font color=&quot;blue&quot;&gt;&amp;lt;DrawRend::render_barycentric_triangle()&lt;/font&gt;. Then, these are passed to &lt;font color=&quot;blue&quot;&gt; ColorTri::color()&lt;/font&gt;. This method then multiplies alpha by the &quot;a&quot; vertex color, beta by the &quot;b&quot; vertex color, and gamma = (1 - alpha - beta) by the &quot;c&quot; vertex co
                    &lt;img src=&quot;../images/part_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Color Wheel Result.&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 6: Pixel sampling for texture mapping&lt;/h2&gt;
&lt;h4&gt; Finding the UV Texture Coordinates &lt;/h4&gt;
&lt;p&gt; Implementing part 6 was very similar to part 5. The same way I used barycentric coordinates to interpolate the colors of three vertices of a triangle for the pixels in the triangle, I could map a coordinate inside a triangle to its corresponding uv texture coordinate. In &lt;font color=&quot;blue&quot;&gt; Color TexTri::color(Vector2D xy, Vector2D dx, Vector2D dy, SampleParams sp) &lt;/font&gt;, I multiply alpha by the &quot;a&quot; vertex&#39;s uv coordinate vector, beta by the &quot;b&quot; coordinate uv vector, and gamma by the &quot;c&quot; coordinate uv vector. &lt;/p&gt;
&lt;h4&gt; Retrieving the Color from the MipMap &lt;/h4&gt;
Once I had adjusted the uv vector, it is passed into &lt;font color=&quot;blue&quot;&gt;Texture::sample(const SampleParams &amp;amp;sp)&lt;/font&gt;, which then calls the appropriate sampling method according to the sp.psm parameter. The sampling method retreives the level 0 mip map (for this portion). The uv coordinates are between 0 and 1, so the sampling method then scales them up to match the proportions of the mipmap. 
&lt;pre align=&quot;center&quot;&gt; x = uv.x * mipmap.width &lt;br /&gt; y = uv.y * mipmap*height &lt;/pre&gt; Then, the color of the pixel is retrieved from the mipmap. 
&lt;pre align=&quot;center&quot;&gt;return MipMapPixelColor(x, y, level)&lt;/pre&gt;
&lt;h4&gt; Nearest vs Bilinear Sampling &lt;/h4&gt;
The nearest level sampling method simply returns the color of the pixel as above. The Bilinear method returns an interpolation of the four nearest uv coordinates. I used the algorithm in the book to implement bilinear sampling. The difference between nearest and bilinear sampling is the most apparent in the map sample images, which have distinct thin vertical lines through them. &lt;/p&gt;

&lt;img src=&quot;../images/part_6_nearest_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 1. &lt;br /&gt;Jaggies! Blegh!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_bilinear_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 1 &lt;br /&gt;
               		SoooOossosoSo smooth &amp;lt;3 &lt;/figcaption&gt;&lt;br /&gt;
Even with a high sample rate, the difference between the two methods is apparent on these images. &lt;br /&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_nearest_16.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 16 &lt;br /&gt;
                    	Gross! Blurry Jaggies!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_bilinear_16.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 16 &lt;br /&gt; 
                   Ridiculously good looking.&lt;/figcaption&gt;&lt;br /&gt;

&lt;br /&gt;&lt;br /&gt;
&lt;p&gt; 
	However, without these distinct lines, the effect is much less noticeable. In the campanille image, I actually prefer the nearest sampling method. The features of the image seem slightly better with the nearest sampling.  &lt;/p&gt;

&lt;img src=&quot;../images/part_6_camp_nearest.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level Pixel Sampling, sample_rate = 16 &lt;br /&gt;
                    	Looks pretty nice.&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part6_camp_bi.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Bilinear Pixel Sampling, sample_rate = 16 &lt;br /&gt; 
                    &lt;/figcaption&gt;&lt;br /&gt;
&lt;p&gt; 
	Initially when retrieving the mip map colors, I had a bug because I wasn&#39;t dividing my colors by 255. Thanks to piazza, I knew I had to divide them by 255 since the color was expecting a float between 0 and 1. However at first that was just giving me black images. Then I realized I literally had to divide it by &quot;255.&quot; to prevent a precision error. 
                    &lt;img src=&quot;../images/part_6_1.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Weird Colors&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_5.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Looks Like Sprinkles!&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_6_4.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Corner Sprinkles&lt;/figcaption&gt;&lt;br /&gt;


&lt;h2 align=&quot;middle&quot;&gt;Part 7: Level sampling with mipmaps for texture mapping&lt;/h2&gt;
&lt;p&gt;In the final part, I implemented the &quot;get level&quot; method using the math described in the textbook. This allows the texture sampling to grab different mip maps for each pixel, depending on which is most appropriate. Sometimes the effects are desirable, and other times it ends up blurring the image. &lt;/p&gt;
&lt;p&gt; In trilinear sampling, the color from the getLevel() mipMap and the color from the adjacent level are blended to give the resulting color. 
                    &lt;img src=&quot;../images/part_7_lzero_pnear.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Nearest Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_7_lzero_plin.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Linear Sampling &lt;br /&gt; &lt;/figcaption&gt;&lt;br /&gt;
                    &lt;p&gt; I thought the above combination, Level Zero with Linear Sampling, gave the best result. It is the only one that eliminates the Jaggies in the &quot;Maleficent&quot; chrome text. &lt;/p&gt;
                    &lt;img src=&quot;../images/part_7_lnear_plin.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Linear Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    &lt;img src=&quot;../images/part_7_lnear_pnear.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Nearest Sampling&lt;/figcaption&gt;&lt;br /&gt;
&lt;p&gt; For closer images, there was visable blurring when using the trilinear sampling met
                    &lt;img src=&quot;../images/part_7_llin_maleficent.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Trilinear Sampling &lt;br /&gt; Observe the blurring in Maleficent&#39;s face. &lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part_7_llin_maleficent2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Trilinear Sampling&lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part7_lnear_pnear_2.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Nearest Level, Nearest Sampling&lt;br /&gt;I felt this was the best result for this image. &lt;/figcaption&gt;&lt;br /&gt;
                    
                    &lt;img src=&quot;../images/part_7_plin_lzero.png&quot; width=&quot;800px&quot; /&gt;
                    &lt;figcaption align=&quot;middle&quot;&gt; Level Zero, Linear Sampling &lt;br /&gt; The jaggies are less apparent in the writing, but Maleficent is a little more blurry.&lt;/figcaption&gt;&lt;br /&gt;



            



&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/td&gt;&lt;/td&gt;&lt;/td&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 28 Jan 2016 00:00:00 -0800</pubDate>
        <link>http://kadie16.github.io/rasterizester/</link>
        <guid isPermaLink="true">http://kadie16.github.io/rasterizester/</guid>
        
        
        <category>featured</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>3D Viewer</title>
        <description>&lt;center&gt;&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/WWfwJuYsd7c&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;p&gt;A demo of my completed 3D Viewer project, written in C++. I also used OpenGL, GLSL, CGAL, and QT. I implemented user interactions including rotation, translation, and zoom by manipulating the viewing volume. The program also supports generation of 3-D volume mesh from surface mesh data.&lt;/p&gt;

&lt;p&gt;Just to clarify, my project is the viewing program itself! Not the models you see. I got the models I used to test my program from the UC Berkeley Computer Graphics group.&lt;/p&gt;

&lt;p&gt;If you are interested in learning about the progression of my work on this project, check out these posts: &lt;br /&gt;
- &lt;a href=&quot;http://kadie.me/working-I-swear/&quot;&gt;Im working, I swear!&lt;/a&gt; &lt;br /&gt;
- &lt;a href=&quot;http://kadie.me/project-update-short/&quot;&gt;Work in Progress&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Aug 2015 00:00:00 -0700</pubDate>
        <link>http://kadie16.github.io/3dviewer/</link>
        <guid isPermaLink="true">http://kadie16.github.io/3dviewer/</guid>
        
        <category>C++</category>
        
        
        <category>projects</category>
        
        <category>Project</category>
        
      </item>
    
      <item>
        <title>3D Viewer</title>
        <description>&lt;center&gt;&lt;iframe width=&quot;420&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/WWfwJuYsd7c&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/center&gt;

&lt;p&gt;A demo of my completed 3D Viewer project, written in C++. I also used OpenGL, GLSL, CGAL, and QT. I implemented user interactions including rotation, translation, and zoom by manipulating the viewing volume. The program also supports generation of 3-D volume mesh from surface mesh data.&lt;/p&gt;

&lt;p&gt;Just to clarify, my project is the viewing program itself! Not the models you see. I got the models I used to test my program from the UC Berkeley Computer Graphics group.&lt;/p&gt;

&lt;p&gt;If you are interested in learning about the progression of my work on this project, check out these posts: &lt;br /&gt;
- &lt;a href=&quot;http://kadie.me/working-I-swear/&quot;&gt;Im working, I swear!&lt;/a&gt; &lt;br /&gt;
- &lt;a href=&quot;http://kadie.me/project-update-short/&quot;&gt;Work in Progress&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 28 Aug 2015 00:00:00 -0700</pubDate>
        <link>http://kadie16.github.io/3dviewer/</link>
        <guid isPermaLink="true">http://kadie16.github.io/3dviewer/</guid>
        
        <category>C++</category>
        
        
        <category>featured</category>
        
        <category>Project</category>
        
      </item>
    
  </channel>
</rss>
